services:
  # Proxy
  proxy: 
    profiles: ["dev"]
    image: haproxy:3.0-bookworm
    # entrypoint: ["/bin/sh", "-c", "sleep 1000"]
    ports:
      - 80:80
      - 443:443
      - 8404:8404
    volumes:
      - ./infrastructure/docker-compose/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ${HOME}/ssl/my-app.pem:/etc/ssl/my-app.pem
    depends_on:
      web:
        condition: service_healthy
  # Backend
  web:
    profiles: ["prod", "dev"]
    #image: aimeric/axum-demo:0.0.1
    # image: axum-demo
    build: 
      dockerfile: Dockerfile.dev
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8080/health/live || exit 1
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 5s
    ports:
      - 8080:8080
    environment:
      SERVICE_DB_HOST: db
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
    volumes:
      - .:/app
    # tty: true
    # stdin_open: true
    depends_on:
      db:
        condition: service_healthy

  # Mandatory Services
  db:
    profiles: ["dev", "prod"]
    image: postgres:15
    user: postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 5s
      timeout: 5s
      retries: 10
    ports:
      - 5432:5432
    environment:
      POSTGRES_PASSWORD: "welcome"
      POSTGRES_DB: "app_db"
    volumes:
      - ./infrastructure/docker-compose/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro

  # Telemetry Components
  otel-collector:
    profiles: ["dev", "prod"]
    image: otel/opentelemetry-collector-contrib:0.119.0
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:13133 || exit 1
    volumes:
      - ./infrastructure/docker-compose/otel/otel-collector.yaml:/etc/otelcol-contrib/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro # Monitor Docker metrics
      # - /proc:/hostfs/proc # Monitor System metrics (Linux Only)
    ports:
      - 4317:4317 # OTLP gRPC receiver
      - 8888:8888 # Prometheus metrics exposed by the collector
      - 13133:13133 # health_check extension
      - 55679:55679 # zpages
    environment:
      GRAFANA_CLOUD_OTLP_ENDPOINT: ${GRAFANA_CLOUD_OTLP_ENDPOINT:-default}
      GRAFANA_CLOUD_INSTANCE_ID: ${GRAFANA_CLOUD_INSTANCE_ID:-default}
      GRAFANA_CLOUD_API_KEY: ${GRAFANA_CLOUD_API_KEY:-default}

  prometheus:
    profiles: ["dev", "prod"]
    image: prom/prometheus:v3.1.0
    ports:
      - 9090:9090
      - 1234:1234
    command:
      - --web.enable-otlp-receiver
      - --web.enable-remote-write-receiver
      - --config.file=/etc/prometheus/prometheus.yml
      - --enable-feature=native-histograms
    volumes:
      - ./infrastructure/docker-compose/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro

  jaeger:
    profiles: ["dev", "prod"]
    image: jaegertracing/all-in-one:1.66.0
    ports:
      - 16686:16686 # Web UI
      - 4316:4317 # Listen for traces over GRCP (different port on host as 4317 is alredy taken by the otel collector)

  grafana:
    profiles: ["dev", "prod"]
    image: grafana/grafana:11.5.1
    ports:
      - 3000:3000
    environment:
      GF_USERS_DEFAULT_THEME: "light"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Admin"
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_BASIC_ENABLED: "false"
    volumes:
      - ./infrastructure/docker-compose/grafana:/etc/grafana/provisioning:ro

  # Tests
  k6:
    profiles: ["test"]
    image: grafana/k6:0.57.0
    volumes:
      - ./infrastructure/docker-compose/k6:/home/k6
    user:
      root
    entrypoint: ["/bin/sh", "-c"]
    command: ["k6 run ${K6_FILE:-script.js}"]
    environment:
      K6_PROMETHEUS_RW_SERVER_URL: "http://prometheus:9090/api/v1/write"
      K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM: "true"
      K6_FILE: ${K6_FILE:-script.js}
